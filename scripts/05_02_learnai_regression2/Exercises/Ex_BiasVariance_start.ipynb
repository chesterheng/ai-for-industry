{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 2 : Understanding Bias/Variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In Machine Learning, we are trying to mimic the system that is generating the data as best as we can. Unless it is a very simple sytem, most likely we would still only get an imperfect model regardless of how many experiments we try. One of the more common challenges for someone new to Machine Learning is how to select the best model during this experimentation process.\n",
    "\n",
    "Let's begin by looking at the prediction error. This is the error between the values generated by our model and the actual outcome values from the system. This error can be expressed simplistically in the formula below (the actual form of the error composition is dependent on the error calculated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Total Error = Bias Error + Variance Error + Irreducible Error$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will focus on the bias & variance error.\n",
    "\n",
    "According to [this](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) Wikipedia entry, **Bias Error** is the error from the errorneous assumptions of the learning algorithm. Whereas **Variance Error** is the error from sensitivity to small fluctuations in the training set.\n",
    "\n",
    "In building models, these 2 errors are in direct conflict with each other that is, minimizing the bias error will increase the variance error and vice versa. In trying to optimize our model and minimizing the overall error, we are always fighting to balance between the 2. \n",
    "\n",
    "A model with high bias and low variance typically correspond to *underfitting* i.e. a model that doesn't not perform well even on training data. However it should give similar models for two randomly chosen training datasets. \n",
    "\n",
    "On the other hand, *overfitting* is where the model performs very well on the training data but produces poor results on new data. This type of model is typically characterized by high variance and low bias since any two training datasets would lead to very different models.\n",
    "\n",
    "Let's see how this idea applies in the model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "We start by importing the libraries needed for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # plot formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a shortcut function to help build a polynomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the housing dataset and check that the data is consistent as in previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./house_prices/train.csv\"\n",
    "input_data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head() # check that the first few rows are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape # check that the overall dataset size is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us illustrate our concepts, let's filter the dataset down to make things simpler. Here, we select only 2 columns and keep only 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['OverallQual','SalePrice']\n",
    "data = input_data[features].sample(n=10, random_state=42)\n",
    "data.columns = ['X','Y'] # rename the columns to make it easier to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a scatter plot to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.X, data.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build 2 models: A polynomial of degree 1 (i.e. a Linear Regression) and another of degree 20. We plot the models side-by-side so that we are able to visualize how they look with our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.X.values.reshape(-1,1)\n",
    "y = data.Y.values\n",
    "\n",
    "# Here we define 2 models: Linear and a higher order polynomial\n",
    "model1 = PolynomialRegression(1).fit(X, y)\n",
    "model20 = PolynomialRegression(20).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xfit is a series of generated x-axis values within the range. This will allow us to plot a smooth function on the graph to represent the model.\n",
    "xfit = np.linspace(X.min(), X.max(), 100)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the results of the 2 models side by side\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# Left subplot\n",
    "ax[0].scatter(X, y, s=40)\n",
    "ax[0].plot(xfit, model1.predict(xfit), color='gray')\n",
    "ax[0].set_title('High-bias model: Underfits the data', size=14)\n",
    "\n",
    "# Right subplot\n",
    "ax[1].scatter(X, y, s=40)\n",
    "ax[1].plot(xfit, model20.predict(xfit), color='gray')\n",
    "ax[1].set_title('High-variance model: Overfits the data', size=14)\n",
    "ax[1].set_ylim(0,350000) # Set the y axis limit, comment out to see the full picture of the model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the high-variance (overfitted) model passes close to or even through the data points in our training dataset.\n",
    "\n",
    "Let's take a look at the training data errors. For this we consider the 2 common regression errors that we have used in the previous lesson i.e. Root Means Square Error (RMSE) and the Coefficient of Determination (R<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 2 error metrics using both lower and higher order polynomials.\n",
    "model1_rmse = np.sqrt(mean_squared_error(y, model1.predict(X)))\n",
    "model1_r2 = r2_score(y, model1.predict(X))\n",
    "print(\"Lower Order Model Errors, RMSE = {:.5}, R2 = {:.5}\".format(model1_rmse, model1_r2))\n",
    "\n",
    "model20_rmse = np.sqrt(mean_squared_error(y, model20.predict(X)))\n",
    "model20_r2 = r2_score(y, model20.predict(X))\n",
    "print(\"Higher Order Model Errors, RMSE = {:.5}, R2 = {:.5}\".format(model20_rmse, model20_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the higher order model gives an almost perfect fit and a very small error on the training datasets.\n",
    "\n",
    "Let's try our models on some unseen data. We sample the original dataset again taking note to take only datapoints within the same range as our training data. We then recalculate the metrics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling new data points from the original dataset. These points must be within the same range as the training data to be a fair comparison.\n",
    "tmp_data = input_data[features]\n",
    "tmp_data.columns = ['X','Y']\n",
    "tmp_data = tmp_data[(tmp_data.X > X.min()) & (tmp_data.X < X.max()) & (tmp_data.Y > y.min()) & (tmp_data.Y < y.max())].sample(n=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_rmse_test = np.sqrt(mean_squared_error(tmp_data.Y, model1.predict(tmp_data.X.values.reshape(-1,1))))\n",
    "model1_r2_test = r2_score(tmp_data.Y, model1.predict(tmp_data.X.values.reshape(-1,1)))\n",
    "print(\"Lower Order Model Errors, RMSE = {:.5}, R2 = {:.5}\".format(model1_rmse_test, model1_r2_test))\n",
    "\n",
    "model20_rmse_test = np.sqrt(mean_squared_error(tmp_data.Y, model20.predict(tmp_data.X.values.reshape(-1,1))))\n",
    "model20_r2_test = r2_score(tmp_data.Y, model20.predict(tmp_data.X.values.reshape(-1,1)))\n",
    "print(\"Higher Order Model Errors, RMSE = {:.5}, R2 = {:.5}\".format(model20_rmse_test, model20_r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of our models give poor results when using the test data as shown by the negative R<sup>2</sup> score. However, note how much worse the higher order model is relative to the training dataset results. The rmse is of one order higher and the R<sup>2</sup> score is much worse. This is a classical indicator of an overfitted model. We can visually confirm the results by adding test data points to the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the results of the 2 models side by side\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# Left subplot\n",
    "ax[0].scatter(X, y, s=40)\n",
    "ax[0].scatter(tmp_data.X, tmp_data.Y, s=40)\n",
    "\n",
    "ax[0].plot(xfit, model1.predict(xfit), color='gray')\n",
    "ax[0].set_title('High-bias model: Underfits the data', size=14)\n",
    "\n",
    "# Right subplot\n",
    "ax[1].scatter(X, y, s=40)\n",
    "ax[1].scatter(tmp_data.X, tmp_data.Y, s=40)\n",
    "ax[1].plot(xfit, model20.predict(xfit), color='gray')\n",
    "ax[1].set_title('High-variance model: Overfits the data', size=14)\n",
    "ax[1].set_ylim(0, 350000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is not surprising that the best model in this simple exercise is somewhere in between the 2. So how do we tune this hyperparameter (polynomial order) so that we obtain the best model?\n",
    "\n",
    "We can try replicate our steps above by fitting multiple models and calculating the errors for each of them. Let's visualize the polynomials against our training data for a few values of the hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(X, y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in range(1,10,2):\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(xfit)\n",
    "    plt.plot(xfit, y_test, label='degree={0}'.format(degree))\n",
    "plt.legend(loc='best');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not the most efficient way to do this. We also want to minimize the exposure of the model to the test data so as to minimize accidentally introducing any bias. \n",
    "\n",
    "Fortunately, scikit has a solution for us! The <code>validation_curve</code> function uses cross-validation on the training dataset to calculate metrics on a training and validation set. We can then visualize this to help us identify the best possible values.\n",
    "\n",
    "The next code block shows how to use this function. We have resampled a larger quantity of data to make the calculation more stable.\n",
    "\n",
    "Note how the validation scores increase in line with the training score then drops off abruptly as the model starts to overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "data = input_data[features].sample(n=50, random_state=42)\n",
    "data.columns = ['X','Y']\n",
    "X = data.X.values.reshape(-1,1)\n",
    "y = data.Y.values\n",
    "\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=5)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the concept behind bias-variance tradeoff, you should be able to take this into account when you are building your models. Happy Experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
