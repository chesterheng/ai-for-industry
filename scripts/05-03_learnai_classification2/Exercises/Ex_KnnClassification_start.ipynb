{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification 2\n",
    "\n",
    "## Exercise 2: Classification using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this exercise, we will begin our classification journey by building a baseline model using KNN. KNN is a simple, easy to understand method that is also very easy to use. It is a nonparametric algorithm that does not make any deep underlying assumption of the data. As such, they are free to 'learn' from the data without restriction. However they do have disadvantages such as:\n",
    "\n",
    "- More data: Require a lot more training data to estimate the mapping function.\n",
    "- Slower: A lot slower to train as they often have far more parameters to train.\n",
    "- Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.\n",
    "\n",
    "[This](https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/) blog article explains this further.\n",
    "\n",
    "To do this exercise, you will need to have completed Exercise 1 and use the data saved from that. Complete the tasks in the text or in the code comments. You will also need to refer to the [Scikit documentation](https://scikit-learn.org/stable/documentation.html).\n",
    "\n",
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Import the relevant Scikit-learn functions and Classes as required. You may have to keep modifying this cell as you discover more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Read in the CSV file saved from Exercise 1\n",
    "file_path = # Filename\n",
    "# The 1st column of the csv file should be the customer ID, which is loaded in the the Dataframe's index\n",
    "input_data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that data is as expected\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of data - TASK: Validate that it's (7032, 20)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome variable\n",
    "# Instead of keeping the values, we will encode as 1s and 0s using the map function\n",
    "output_var_name = 'ChurnLabel'\n",
    "output_var = input_data[output_var_name]\n",
    "output_var = output_var.map({'Yes': 1, 'No': 0})\n",
    "# Note that the map function can be run only once. You will get an error if you try to run this cell again as Yes/No are no longer valid values in this feature. \n",
    "\n",
    "# Count the number of rows for each outcome value\n",
    "print(\"Row count for each outcome\")\n",
    "print(output_var.value_counts())\n",
    "\n",
    "# Remove the outcome variable from the main dataframe\n",
    "input_data.drop(output_var_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to define 3 lists for each of the data types found in our data i.e. Numerical, Categorical (more than 2 values), Binary (2 values only)\n",
    "\n",
    "# Numerical features\n",
    "num_features = [key for key in dict(input_data.dtypes) if dict(input_data.dtypes)[key] in ['int64', 'float64']]\n",
    "print(num_features) # TASK: Confirm the columns based on Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Define the 4 categorical features as a list of strings. These are the non-numerical features that do not have Yes/No values\n",
    "cat_features = # Categorical feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Define the binary features. Complete the steps denoted in this cell.\n",
    "# 1. Get the list of non-numerical features (both categorical and binary). Hint: Add 'not' to the code from num_features\n",
    "bin_features = # Copy then modify the code from num_features\n",
    "\n",
    "# 2. Remove the categorical feature names from this list\n",
    "for col in cat_features:\n",
    "    # Hint: There is a list method to remove an element\n",
    "print(f\"List of binary features: {bin_features}\") # TASK: Confirm the resulting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the binary features. Similar to the outcome variable, we will need to convert the values of these features from Yes/No to 1/0.\n",
    "# Note: As an alternative, this could have been done when building the pipeline.\n",
    "# TASK: Complete the code \n",
    "for col in bin_features:\n",
    "    input_data[col] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display values after encoding\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline. Reminder that the binary features have already been encoded and thus only passed through\n",
    "# TASK: Match the list of features to the correct encoding operation. \n",
    "# Remember to add the library imports for ColumnTransformer, StandardScaler, OneHotEncoder to the imports above\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('standardscaler', StandardScaler(), ),\n",
    "        ('onehotencoder', OneHotEncoder(), )\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Complete the pipeline by adding the KNN Algorithm i.e. KNeighborClassifier\n",
    "# At this moment, use the n_neighbors=5 as the parameter to KNeighborClassifier. Once this script is working, feel free to try out other values.\n",
    "model = make_pipeline(\n",
    "    preprocess,\n",
    "    # TASK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "# TASK: Split the data into 70:30 train/test. Use the random_state=42\n",
    "x_train, x_test, y_train, y_test = # TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the data. TASK: Confirm as (2110, 19)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline. You can add a semi-colon (';') at the end of the line to supresses the output printing\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For regression problems, we are familiar with common metrics such as Root Means Square Error (RMSE) and the Coefficient of Determination (R<sup>2</sup> value).\n",
    "\n",
    "With classification problems, we need a different set of metrics to evaluate the model. Here, we use metrics such as:\n",
    "\n",
    "- Confusion Matrix\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- ROC and AUC\n",
    "\n",
    "Read the following blog posts to get familiar with these terms:\n",
    "\n",
    "- https://hackernoon.com/idiots-guide-to-precision-recall-and-confusion-matrix-b32d36463556\n",
    "- https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n",
    "Once you are done, proceed to the next cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test data\n",
    "pred_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Import the following metrics from scikit-learn in the library imports section above\n",
    "# confusion_matrix, accuracy_score, precision, recall, f1_score, classification_report, roc_curve, auc\n",
    "# Note that all these functions have the same parameter profile i.e. the first parameter contains the actual values while the second parameter contains the predicted values from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Calculate the confusion matrix\n",
    "cm = # TASK\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices typically are displayed in a graphical manner. Run this cell to display the matrix using this code snippet found online.\n",
    "labels = ['No','Yes']\n",
    "ax= plt.subplot()\n",
    "sn.heatmap(cm, annot=True, ax = ax, fmt=\"d\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Calculate the 4 following metrics using the correct function\n",
    "# OPTIONAL: Calculate these metrics by hand (using the formulas in the reference blog posts) to validate the values\n",
    "\n",
    "# 1. Accuracy = Sum of correctly predicted outcomes divided by total number of samples\n",
    "accuracy = # TASK\n",
    "print(\"Accuracy: {:.5f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Precision - Of those predicted positive, how many of them are actual positive.\n",
    "precision = # TASK\n",
    "print(\"Precision: {:.5f}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Recall - how many of the actual positives our model is predicting as positives\n",
    "recall = # TASK\n",
    "print(\"Recall: {:.5f}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. F1 score\n",
    "f1 = #TASK\n",
    "print(\"F1 Score: {:.5f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can calculate all these metrics in one call using the classification_report function\n",
    "print(classification_report(y_test, pred_test, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, take a step back and try to understand these numbers. \n",
    "\n",
    "- What do these metrics mean in the context of the problem? \n",
    "- Which error (Type 1 or Type 2) is more important for this problem? And thus which metric is more important, Precision or Recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Compute ROC and AUC. Note that roc_curve() returns 3 values. You will only need the first 2 as input to auc() i.e. use _ as the 3rd output\n",
    "fpr, tpr, _ = # TASK\n",
    "#TASK = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have built a basic classication model. Complete the lesson quiz and proceed to the next lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
