{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-02-preprocessing-for-machine-learning-in-python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPnTMD3IY7Cj"
      },
      "source": [
        "#Preprocessing for Machine Learning in Python\n",
        "## 1. Introduction to Data Preprocessing\n",
        "## 2. Standardizing Data\n",
        "## 3. Feature Engineering\n",
        "## 4. Selecting features for modeling\n",
        "## 5. Putting it all together"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p2IJpOOb973"
      },
      "source": [
        "## 1. Introduction to Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8a67gEfg-w",
        "outputId": "49b990a7-c5ed-4404-a920-1f629f649f30"
      },
      "source": [
        "# What is data preprocessing?\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "print(volunteer.shape)\n",
        "\n",
        "volunteer = volunteer.dropna(axis=1, thresh=3)\n",
        "print(volunteer.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(665, 35)\n",
            "(665, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqWk_cFWcJMd"
      },
      "source": [
        "**Missing data - columns**\n",
        "\n",
        "We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.\n",
        "\n",
        "How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?\n",
        "\n",
        "- The dataset volunteer has been provided.\n",
        "- Use the dropna() function to remove columns.\n",
        "- You'll have to set both the axis= and thresh= parameters.\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [x] 35, 24\n",
        "- [ ] 35, 35\n",
        "- [ ] 35, 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vQCKvRDcD7c",
        "outputId": "d1691be2-7b30-48a8-f808-46132d38308b"
      },
      "source": [
        "# Missing data - rows\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "\n",
        "# Check how many values are missing in the category_desc column\n",
        "print(volunteer['category_desc'].isnull().sum())\n",
        "volunteer['category_desc'].head()\n",
        "\n",
        "# Subset the volunteer dataset\n",
        "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
        "\n",
        "# Print out the shape of the subset\n",
        "print(volunteer_subset.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n",
            "(617, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s19YkF5GsVyy",
        "outputId": "53f9f9d1-cfa0-4f06-9f8e-19654352e040"
      },
      "source": [
        "# Working with data types\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "volunteer.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "opportunity_id          int64\n",
              "content_id              int64\n",
              "vol_requests            int64\n",
              "event_time              int64\n",
              "title                  object\n",
              "hits                    int64\n",
              "summary                object\n",
              "is_priority            object\n",
              "category_id           float64\n",
              "category_desc          object\n",
              "amsl                  float64\n",
              "amsl_unit             float64\n",
              "org_title              object\n",
              "org_content_id          int64\n",
              "addresses_count         int64\n",
              "locality               object\n",
              "region                 object\n",
              "postalcode            float64\n",
              "primary_loc           float64\n",
              "display_url            object\n",
              "recurrence_type        object\n",
              "hours                   int64\n",
              "created_date           object\n",
              "last_modified_date     object\n",
              "start_date_date        object\n",
              "end_date_date          object\n",
              "status                 object\n",
              "Latitude              float64\n",
              "Longitude             float64\n",
              "Community Board       float64\n",
              "Community Council     float64\n",
              "Census Tract          float64\n",
              "BIN                   float64\n",
              "BBL                   float64\n",
              "NTA                   float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4a2RBSnsbTw"
      },
      "source": [
        "**Exploring data types**\n",
        "\n",
        "Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we'll be working with as we start to do more preprocessing.\n",
        "\n",
        "Which data types are present in the volunteer dataset?\n",
        "\n",
        "The dataset volunteer has been provided.\n",
        "Use the .dtypes attribute to check the datatypes.\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] Float and int only\n",
        "- [ ] Int only\n",
        "- [x] Float, int, and object\n",
        "- [ ] Float only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKG68pWHs2EU",
        "outputId": "b496ccbf-b3f2-426a-eeae-e5b6bd20331d"
      },
      "source": [
        "# Converting a column type\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "\n",
        "# Print the head of the hits column\n",
        "print(volunteer['hits'].head())\n",
        "\n",
        "# Convert the hits column to type int\n",
        "volunteer['hits'] = volunteer['hits'].astype(int)\n",
        "\n",
        "# Look at the dtypes of the dataset\n",
        "print(volunteer.dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    737\n",
            "1     22\n",
            "2     62\n",
            "3     14\n",
            "4     31\n",
            "Name: hits, dtype: int64\n",
            "opportunity_id          int64\n",
            "content_id              int64\n",
            "vol_requests            int64\n",
            "event_time              int64\n",
            "title                  object\n",
            "hits                    int64\n",
            "summary                object\n",
            "is_priority            object\n",
            "category_id           float64\n",
            "category_desc          object\n",
            "amsl                  float64\n",
            "amsl_unit             float64\n",
            "org_title              object\n",
            "org_content_id          int64\n",
            "addresses_count         int64\n",
            "locality               object\n",
            "region                 object\n",
            "postalcode            float64\n",
            "primary_loc           float64\n",
            "display_url            object\n",
            "recurrence_type        object\n",
            "hours                   int64\n",
            "created_date           object\n",
            "last_modified_date     object\n",
            "start_date_date        object\n",
            "end_date_date          object\n",
            "status                 object\n",
            "Latitude              float64\n",
            "Longitude             float64\n",
            "Community Board       float64\n",
            "Community Council     float64\n",
            "Census Tract          float64\n",
            "BIN                   float64\n",
            "BBL                   float64\n",
            "NTA                   float64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIDmMmp0tH_m",
        "outputId": "377773f2-4353-4c00-ed14-f9c8df83d28e"
      },
      "source": [
        "# Class distribution\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "volunteer['category_desc'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Strengthening Communities    307\n",
              "Helping Neighbors in Need    119\n",
              "Education                     92\n",
              "Health                        52\n",
              "Environment                   32\n",
              "Emergency Preparedness        15\n",
              "Name: category_desc, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bxtK_-3tVJ8"
      },
      "source": [
        "**Class imbalance**\n",
        "\n",
        "In the volunteer dataset, we're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.\n",
        "\n",
        "Which descriptions occur less than 50 times in the volunteer dataset?\n",
        "\n",
        "- The dataset volunteer has been provided.\n",
        "- The colum you want to check is category_desc.\n",
        "- Use the value_counts() method to check variable counts.\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] Emergency Preparedness\n",
        "- [ ] Health\n",
        "- [ ] Environment\n",
        "- [x] 1 and 3\n",
        "- [ ] All of the above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1LO1f1Tt5dI",
        "outputId": "e831b1f4-8dbb-4306-b672-c9337d570691"
      },
      "source": [
        "# Stratified sampling\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "volunteer = volunteer[volunteer['category_desc'].notnull()]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a data with all columns except category_desc\n",
        "volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
        "\n",
        "# Create a category_desc labels dataset\n",
        "volunteer_y = volunteer[[\"category_desc\"]]\n",
        "\n",
        "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
        "\n",
        "# Print out the category_desc counts on the training y labels\n",
        "print(y_train[\"category_desc\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Strengthening Communities    230\n",
            "Helping Neighbors in Need     89\n",
            "Education                     69\n",
            "Health                        39\n",
            "Environment                   24\n",
            "Emergency Preparedness        11\n",
            "Name: category_desc, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_sLsSerxshR"
      },
      "source": [
        "## 2. Standardizing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AV2zAtGxvwY"
      },
      "source": [
        "**When to standardize**\n",
        "\n",
        "Now that you've learned when it is appropriate to standardize your data, which of these scenarios would you NOT want to standardize?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] A column you want to use for modeling has extremely high variance.\n",
        "- [ ] You have a dataset with several continuous columns on different scales and you'd like to use a linear model to train the data.\n",
        "- [ ] The models you're working with use some sort of distance metric in a linear space, like the Euclidean metric.\n",
        "- [x] Your dataset is comprised of categorical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSmBr4b6xsP9",
        "outputId": "2fbcb65e-19a4-48d5-d1b4-5ead796da000"
      },
      "source": [
        "# Modeling without normalizing\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "y = wine.Type\n",
        "X = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Split the dataset and labels into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6888888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ39IMW2zAWi",
        "outputId": "2b5566d8-d1f6-48c1-f0af-ad0aa598281d"
      },
      "source": [
        "# Log normalization\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "wine.var()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Type                                0.600679\n",
              "Alcohol                             0.659062\n",
              "Malic acid                          1.248015\n",
              "Ash                                 0.075265\n",
              "Alcalinity of ash                  11.152686\n",
              "Magnesium                         203.989335\n",
              "Total phenols                       0.391690\n",
              "Flavanoids                          0.997719\n",
              "Nonflavanoid phenols                0.015489\n",
              "Proanthocyanins                     0.327595\n",
              "Color intensity                     5.374449\n",
              "Hue                                 0.052245\n",
              "OD280/OD315 of diluted wines        0.504086\n",
              "Proline                         99166.717355\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXflmYEKzIlq"
      },
      "source": [
        "**Checking the variance**\n",
        "\n",
        "Check the variance of the columns in the wine dataset. Out of the four columns listed in the multiple choice section, which column is a candidate for normalization?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] Alcohol\n",
        "- [x] Proline (Has an extremely high variance.)\n",
        "- [ ] Proanthocyanins\n",
        "- [ ] Ash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtabTUyD0GFH",
        "outputId": "b85f0c45-8172-4a21-84bd-4e473cd76886"
      },
      "source": [
        "# Log normalization in Python\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "# Print out the variance of the Proline column\n",
        "print(wine.Proline.var())\n",
        "\n",
        "# Apply the log normalization function to the Proline column\n",
        "wine['Proline_log'] = np.log(wine.Proline)\n",
        "\n",
        "# Check the variance of the Proline column again\n",
        "print(wine.Proline_log.var())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99166.71735542428\n",
            "0.17231366191842018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "xIuizFib0YE3",
        "outputId": "9585bb6c-714c-404a-9a5e-8e30c651dea4"
      },
      "source": [
        "# Scaling data for feature comparison\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "wine.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315 of diluted wines</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.938202</td>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "      <td>2.366517</td>\n",
              "      <td>19.494944</td>\n",
              "      <td>99.741573</td>\n",
              "      <td>2.295112</td>\n",
              "      <td>2.029270</td>\n",
              "      <td>0.361854</td>\n",
              "      <td>1.590899</td>\n",
              "      <td>5.058090</td>\n",
              "      <td>0.957449</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>746.893258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.775035</td>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "      <td>0.274344</td>\n",
              "      <td>3.339564</td>\n",
              "      <td>14.282484</td>\n",
              "      <td>0.625851</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.572359</td>\n",
              "      <td>2.318286</td>\n",
              "      <td>0.228572</td>\n",
              "      <td>0.709990</td>\n",
              "      <td>314.907474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>278.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "      <td>2.210000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.742500</td>\n",
              "      <td>1.205000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>500.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "      <td>2.360000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>2.355000</td>\n",
              "      <td>2.135000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>4.690000</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>673.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "      <td>2.557500</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>985.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.230000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1680.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Type     Alcohol  ...  OD280/OD315 of diluted wines      Proline\n",
              "count  178.000000  178.000000  ...                    178.000000   178.000000\n",
              "mean     1.938202   13.000618  ...                      2.611685   746.893258\n",
              "std      0.775035    0.811827  ...                      0.709990   314.907474\n",
              "min      1.000000   11.030000  ...                      1.270000   278.000000\n",
              "25%      1.000000   12.362500  ...                      1.937500   500.500000\n",
              "50%      2.000000   13.050000  ...                      2.780000   673.500000\n",
              "75%      3.000000   13.677500  ...                      3.170000   985.000000\n",
              "max      3.000000   14.830000  ...                      4.000000  1680.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnx8arEU0flz"
      },
      "source": [
        "**Scaling data - investigating columns**\n",
        "\n",
        "We want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. \n",
        "\n",
        "Using describe() to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162.\n",
        "- [ ] The means of Ash and Alcalinity of ash are less than 20, while the mean of Magnesium is greater than 90.\n",
        "- [ ] The standard deviations of Ash and Alcalinity of ash are equal.\n",
        "- [x] 1 and 2 are true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fSRP8dM1CI9"
      },
      "source": [
        "# Scaling data - standardizing columns\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "# Import StandardScaler from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaler\n",
        "ss = StandardScaler()\n",
        "\n",
        "# Take a subset of the DataFrame you want to scale \n",
        "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
        "\n",
        "# Apply the scaler to the DataFrame subset\n",
        "wine_subset_scaled = ss.fit_transform(wine_subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTNgMfeS1vPo",
        "outputId": "20e87ea8-b368-4e80-ef55-7d74dd4bf776"
      },
      "source": [
        "# Standardized data and modeling\n",
        "\n",
        "# KNN on non-scaled data\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "X = wine.drop('Type', axis=1)\n",
        "y = wine.Type\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Split the dataset and labels into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7777777777777778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ6f_Gg12saL",
        "outputId": "fbe5635d-2f31-413f-f73d-5ddf5854aecb"
      },
      "source": [
        "# KNN on scaled data\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "X = wine.drop('Type', axis=1)\n",
        "y = wine.Type\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import StandardScaler from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaling method.\n",
        "ss = StandardScaler()\n",
        "\n",
        "# Apply the scaling method to the dataset used for modeling.\n",
        "X_scaled = ss.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data.\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data.\n",
        "print(knn.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9777777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLDN4nuL3MYm"
      },
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBJtB1PT3Oqu"
      },
      "source": [
        "**Feature engineering knowledge test**\n",
        "\n",
        "Now that you've learned about feature engineering, which of the following examples are good candidates for creating new features?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] A column of timestamps\n",
        "- [ ] A column of newspaper headlines\n",
        "- [ ] A column of weight measurements\n",
        "- [x] 1 and 2\n",
        "- [ ] None of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1tJkjNYyWVL"
      },
      "source": [
        "**Identifying areas for feature engineering**\n",
        "\n",
        "Take an exploratory look at the volunteer dataset, using the variable of that name. \n",
        "\n",
        "Which of the following columns would you want to perform a feature engineering task on?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] vol_requests\n",
        "- [ ] title\n",
        "- [ ] created_date\n",
        "- [ ] category_desc\n",
        "- [x] 2, 3, and 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA0pHuvh0e28",
        "outputId": "98454c77-d843-4d77-e7b2-fb7c4ab92794"
      },
      "source": [
        "# Encoding categorical variables\n",
        "\n",
        "# Encoding categorical variables - binary\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'\n",
        "hiking = pd.read_json(filename)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Set up the LabelEncoder object\n",
        "enc = LabelEncoder()\n",
        "\n",
        "# Apply the encoding to the \"Accessible\" column\n",
        "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
        "\n",
        "# Compare the two columns\n",
        "print(hiking[['Accessible', 'Accessible_enc']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Accessible  Accessible_enc\n",
            "0          Y               1\n",
            "1          N               0\n",
            "2          N               0\n",
            "3          N               0\n",
            "4          N               0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbU3yUdw1h5a",
        "outputId": "5ac143b5-d1c8-41d5-a5e1-9657440744e7"
      },
      "source": [
        "# Encoding categorical variables - one-hot\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "\n",
        "# Transform the category_desc column\n",
        "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
        "\n",
        "# Take a look at the encoded columns\n",
        "print(category_enc.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Education  ...  Strengthening Communities\n",
            "0          0  ...                          0\n",
            "1          0  ...                          1\n",
            "2          0  ...                          1\n",
            "3          0  ...                          1\n",
            "4          0  ...                          0\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV3USwxO17fK",
        "outputId": "b5259a27-21e8-49c9-ea51-bed53c6d2bc6"
      },
      "source": [
        "# Engineering numerical features\n",
        "\n",
        "# Engineering numerical features - taking an average\n",
        "\n",
        "data = {'name': ['Sue', 'Mark', 'Sean', 'Erin', 'Jenny', 'Russell'],\n",
        "        'run1': [20.1, 16.5, 23.5, 21.7, 25.8, 30.9],\n",
        "        'run2': [18.5, 17.1, 25.1, 21.1, 27.1, 29.6],\n",
        "        'run3': [19.6, 16.9, 25.2, 20.9, 26.1, 31.4],\n",
        "        'run4': [20.3, 17.6, 24.6, 22.1, 26.7, 30.4],\n",
        "        'run5': [18.3, 17.3, 23.9, 22.2, 26.9, 29.9]}\n",
        "running_times_5k = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Create a list of the columns to average\n",
        "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
        "\n",
        "# Use apply to create a mean column\n",
        "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
        "\n",
        "# Take a look at the results\n",
        "print(running_times_5k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      name  run1  run2  run3  run4  run5   mean\n",
            "0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
            "1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
            "2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
            "3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
            "4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
            "5  Russell  30.9  29.6  31.4  30.4  29.9  30.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE8k0KV33zrG",
        "outputId": "ecfd4e43-7c8c-402d-bd14-5290287f7ddd"
      },
      "source": [
        "# Engineering numerical features - datetime\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "\n",
        "# First, convert string column to date column\n",
        "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
        "\n",
        "# Extract just the month from the converted column\n",
        "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].apply(lambda row: row.month)\n",
        "\n",
        "# Take a look at the original and new columns\n",
        "print(volunteer[['start_date_converted', 'start_date_month']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  start_date_converted  start_date_month\n",
            "0           2011-07-30                 7\n",
            "1           2011-02-01                 2\n",
            "2           2011-01-29                 1\n",
            "3           2011-02-14                 2\n",
            "4           2011-02-05                 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3YbBOWC4C0e",
        "outputId": "714135f7-f515-48f1-91f9-ae6eaf38b295"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "# Engineering features from strings - extraction\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'\n",
        "hiking = pd.read_json(filename)\n",
        "hiking = hiking.dropna(subset=['Length'])\n",
        "\n",
        "import re\n",
        "\n",
        "# Write a pattern to extract numbers and decimals\n",
        "def return_mileage(length):\n",
        "    pattern = re.compile(\"\\d+\\.\\d+\")\n",
        "    print(length)\n",
        "    # Search the text for matches\n",
        "    mile = re.match(pattern, length)\n",
        "    \n",
        "    # If a value is returned, use group(0) to return the found value\n",
        "    if mile is not None:\n",
        "        return float(mile.group(0))\n",
        "\n",
        "      \n",
        "# Apply the function to the Length column and take a look at both columns\n",
        "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
        "print(hiking[[\"Length\", \"Length_num\"]].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8 miles\n",
            "1.0 mile\n",
            "0.75 miles\n",
            "0.5 miles\n",
            "0.5 miles\n",
            "Various\n",
            "1.7 miles\n",
            "2.4 miles\n",
            "1.0 mile\n",
            "3.0 miles\n",
            "12.3 miles\n",
            "0.85 miles\n",
            "4.0 miles\n",
            "7.6 miles\n",
            "8.0 miles\n",
            "0.5 miles\n",
            "7.6 miles\n",
            "0.75 miles\n",
            "0.25 miles\n",
            "12.3 miles\n",
            "1.4 miles\n",
            "1.25 miles\n",
            "1.5 miles\n",
            "1.1 mile\n",
            "1.5 miles\n",
            "1.2 miles\n",
            "0.75 miles\n",
            "1.5 miles\n",
            "3.0 miles\n",
            "       Length  Length_num\n",
            "0   0.8 miles        0.80\n",
            "1    1.0 mile        1.00\n",
            "2  0.75 miles        0.75\n",
            "3   0.5 miles        0.50\n",
            "4   0.5 miles        0.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTaYIZJvTxBZ",
        "outputId": "5ee5f575-2a98-4b4f-cc1c-b15bc40e14df"
      },
      "source": [
        "# Engineering features from strings - tf/idf\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Take the title text\n",
        "title_text = volunteer['title']\n",
        "\n",
        "# Create the vectorizer method\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "\n",
        "# Transform the text into tf-idf vectors\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(665, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xMIJ7PaVZTA",
        "outputId": "14f59bc1-c6fa-4731-ae1d-2ee4388ffcc9"
      },
      "source": [
        "# Text classification using tf/idf vectors\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])\n",
        "volunteer = volunteer.dropna(subset=['category_desc'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "title_text = volunteer['title']\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Split the dataset according to the class distribution of category_desc\n",
        "y = volunteer[\"category_desc\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5096774193548387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYJb-DlTX8p9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvyr7t28X_wu"
      },
      "source": [
        "## 4. Selecting features for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoKYLG0IYAvF"
      },
      "source": [
        "# Feature selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDyTFPvkYGZl"
      },
      "source": [
        "**When to use feature selection**\n",
        "\n",
        "Let's say you had finished standardizing your data and creating new features. \n",
        "\n",
        "Which of the following scenarios is NOT a good candidate for feature selection?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] Several columns of running times that have been averaged into a new column.\n",
        "- [x] A text field that hasn't been turned into a tf/idf vector yet.\n",
        "- [ ] A column of text that has already had a float extracted out of it.\n",
        "- [ ] A categorical field that has been one-hot encoded.\n",
        "- [ ] Your dataset contains columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgTy-KrdZyoR"
      },
      "source": [
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'\n",
        "hiking = pd.read_json(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNWJy4N8Y5mV"
      },
      "source": [
        "**Identifying areas for feature selection**\n",
        "\n",
        "Take an exploratory look at the post-feature engineering hiking dataset. \n",
        "\n",
        "Which of the following columns is a good candidate for feature selection?\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "- [ ] Length\n",
        "- [ ] Difficulty\n",
        "- [ ] Accessible\n",
        "- [x] All of the above\n",
        "- [ ] None of the above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZGvVT8DZ8Z8",
        "outputId": "b2f7f568-2506-4001-b9a5-df7b7a0b2cad"
      },
      "source": [
        "# Removing redundant features\n",
        "\n",
        "# Selecting relevant features\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename)\n",
        "volunteer = volunteer.dropna(subset=['category_desc'])\n",
        "\n",
        "# Create a list of redundant column names to drop\n",
        "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
        "\n",
        "# Drop those columns from the dataset\n",
        "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
        "\n",
        "# Print out the head of the new dataset\n",
        "print(volunteer_subset.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   opportunity_id  content_id  event_time  ... BIN  BBL NTA\n",
            "1            5008       37036           0  ... NaN  NaN NaN\n",
            "2            5016       37143           0  ... NaN  NaN NaN\n",
            "3            5022       37237           0  ... NaN  NaN NaN\n",
            "4            5055       37425           0  ... NaN  NaN NaN\n",
            "5            5056       37426           0  ... NaN  NaN NaN\n",
            "\n",
            "[5 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6GY-Nr8eBQQ",
        "outputId": "78b33cd0-ab78-4f4e-a1cc-ace9ddb1b05f"
      },
      "source": [
        "# Checking for correlated features\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename, usecols=['Flavanoids', 'Total phenols', 'Malic acid', 'OD280/OD315 of diluted wines', 'Hue'])\n",
        "wine = wine.reindex(columns=['Flavanoids', 'Total phenols', 'Malic acid', 'OD280/OD315 of diluted wines', 'Hue'])\n",
        "\n",
        "# Print out the column correlations of the wine dataset\n",
        "print(wine.corr())\n",
        "\n",
        "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
        "to_drop = \"Flavanoids\"\n",
        "\n",
        "# Drop that column from the DataFrame\n",
        "wine = wine.drop(to_drop, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                              Flavanoids  ...       Hue\n",
            "Flavanoids                      1.000000  ...  0.543479\n",
            "Total phenols                   0.864564  ...  0.433681\n",
            "Malic acid                     -0.411007  ... -0.561296\n",
            "OD280/OD315 of diluted wines    0.787194  ...  0.565468\n",
            "Hue                             0.543479  ...  1.000000\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xwYZJk8gSiV",
        "outputId": "fe4f9605-6fc0-4392-e76f-646db1eb4b70"
      },
      "source": [
        "# Selecting features using text vectors\n",
        "\n",
        "# Exploring text vectors, part 1\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])\n",
        "volunteer = volunteer.dropna(subset=['category_desc'])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "title_text = volunteer['title']\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "\n",
        "# Add in the rest of the parameters\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    \n",
        "    # Let's transform that zipped dict into a series\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    \n",
        "    # Let's sort the series to pull out the top n weighted words\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    \n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "# Print out the weighted words\n",
        "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[189, 942, 466]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIWkDsTvgVZE"
      },
      "source": [
        "# Exploring text vectors, part 2\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'\n",
        "volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])\n",
        "volunteer = volunteer.dropna(subset=['category_desc'])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "title_text = volunteer['title']\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "\n",
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "    \n",
        "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "# Call the function to get the list of word indices\n",
        "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
        "\n",
        "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
        "filtered_text = text_tfidf[:, list(filtered_words)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRDWTcKAnt0z",
        "outputId": "b40652b7-7a1f-44eb-8de0-ae3066c60d6a"
      },
      "source": [
        "# Training Naive Bayes with feature selection\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "np = GaussianNB()\n",
        "\n",
        "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
        "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(train_X, train_y)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(test_X, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4838709677419355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zng3y_PMoiAx",
        "outputId": "66d562ee-3cbf-4486-b5c2-a1b8784cfff3"
      },
      "source": [
        "# Dimensionality reduction\n",
        "\n",
        "# Using PCA\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'\n",
        "wine = pd.read_csv(filename)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set up PCA and the X vector for diminsionality reduction\n",
        "pca = PCA()\n",
        "wine_X = wine.drop(\"Type\", axis=1)\n",
        "\n",
        "# Apply PCA to the wine dataset X vector\n",
        "transformed_X = pca.fit_transform(wine_X)\n",
        "\n",
        "# Look at the percentage of variance explained by the different components\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
            " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
            " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
            " 8.25392788e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9tuU0bWpWcn",
        "outputId": "72d29115-012c-44a1-9f0f-430eb8cc448c"
      },
      "source": [
        "# Training a model with PCA\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "knn = GaussianNB()\n",
        "\n",
        "y = wine.dropna().Type\n",
        "transformed_X.shape\n",
        "\n",
        "# Split the transformed X and the y labels into training and test sets\n",
        "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
        "\n",
        "# Fit knn to the training data\n",
        "knn.fit(X_wine_train, y_wine_train)\n",
        "\n",
        "# Score knn on the test data and print it out\n",
        "print(knn.score(X_wine_test, y_wine_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDIHBAMcp8p9"
      },
      "source": [
        "## 5. Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPKOCLCap_PJ",
        "outputId": "9595af3c-a683-4ccf-8ed4-7256e6208eb4"
      },
      "source": [
        "# UFOs and preprocessing\n",
        "\n",
        "# Checking column types\n",
        "\n",
        "import pandas as pd\n",
        "filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/ufo_sightings_large.csv'\n",
        "ufo = pd.read_csv(filename, parse_dates=['date'])\n",
        "\n",
        "# Check the column types\n",
        "print(ufo.dtypes)\n",
        "\n",
        "# Change the type of seconds to float\n",
        "ufo[\"seconds\"] = ufo['seconds'].astype('float')\n",
        "\n",
        "# Change the date column to type datetime\n",
        "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
        "\n",
        "# Check the column types\n",
        "print(ufo[['seconds', 'date']].dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date              datetime64[ns]\n",
            "city                      object\n",
            "state                     object\n",
            "country                   object\n",
            "type                      object\n",
            "seconds                  float64\n",
            "length_of_time            object\n",
            "desc                      object\n",
            "recorded                  object\n",
            "lat                       object\n",
            "long                     float64\n",
            "dtype: object\n",
            "seconds           float64\n",
            "date       datetime64[ns]\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvsCjh4grsDS",
        "outputId": "e2378dac-4407-47bf-80a8-2d9a758b6a4f"
      },
      "source": [
        "# Dropping missing data\n",
        "\n",
        "# Check how many values are missing in the length_of_time, state, and type columns\n",
        "print(ufo[['length_of_time', 'state', 'type']].isnull().sum())\n",
        "\n",
        "# Keep only rows where length_of_time, state, and type are not null\n",
        "ufo_no_missing = ufo[\n",
        "        ufo['length_of_time'].notnull() & \n",
        "        ufo['state'].notnull() & \n",
        "        ufo['type'].notnull()\n",
        "]\n",
        "\n",
        "# Print out the shape of the new dataset\n",
        "print(ufo_no_missing.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length_of_time    143\n",
            "state             419\n",
            "type              159\n",
            "dtype: int64\n",
            "(4283, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWJc2Hftt9XD",
        "outputId": "5ed681d2-cccb-4044-c4fd-679ec5c6402e"
      },
      "source": [
        "# Categorical variables and standardization\n",
        "\n",
        "ufo = ufo_no_missing.copy()\n",
        "import re\n",
        "\n",
        "# Extracting numbers from strings\n",
        "\n",
        "def return_minutes(time_string):\n",
        "    \n",
        "    # We'll use \\d+ to grab digits and match it to the column values\n",
        "    pattern = re.compile(r\"\\d+\")\n",
        "        \n",
        "    # Use match on the pattern and column\n",
        "    num = re.match(pattern, time_string)\n",
        "    if num is not None:\n",
        "        return int(num.group(0))\n",
        "        \n",
        "# Apply the extraction to the length_of_time column\n",
        "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
        "\n",
        "# Take a look at the head of both of the columns\n",
        "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    length_of_time  minutes\n",
            "0          2 weeks      2.0\n",
            "1           30sec.     30.0\n",
            "3  about 5 minutes      NaN\n",
            "4                2      2.0\n",
            "5       10 minutes     10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecs2lBV9mf8H",
        "outputId": "298ba833-e58e-4481-add1-299fea84547e"
      },
      "source": [
        "# Identifying features for standardization\n",
        "\n",
        "ufo.var()\n",
        "import numpy as np\n",
        "\n",
        "# Check the variance of the seconds and minutes columns\n",
        "print(ufo[['seconds','minutes']].var())\n",
        "\n",
        "# Log normalize the seconds column\n",
        "ufo[\"seconds_log\"] = np.log(ufo[['seconds']])\n",
        "\n",
        "# Print out the variance of just the seconds_log column\n",
        "print(ufo[\"seconds_log\"].var())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seconds    1.545212e+10\n",
            "minutes    9.470577e+02\n",
            "dtype: float64\n",
            "nan\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4283 entries, 0 to 4934\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   date            4283 non-null   datetime64[ns]\n",
            " 1   city            4283 non-null   object        \n",
            " 2   state           4283 non-null   object        \n",
            " 3   country         3891 non-null   object        \n",
            " 4   type            4283 non-null   object        \n",
            " 5   seconds         4283 non-null   float64       \n",
            " 6   length_of_time  4283 non-null   object        \n",
            " 7   desc            4283 non-null   object        \n",
            " 8   recorded        4283 non-null   object        \n",
            " 9   lat             4283 non-null   object        \n",
            " 10  long            4283 non-null   float64       \n",
            " 11  minutes         3778 non-null   float64       \n",
            " 12  seconds_log     4283 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(4), object(8)\n",
            "memory usage: 468.5+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIukqzWUp0CN",
        "outputId": "f4698a06-86bc-40b5-b61a-d227a1704660"
      },
      "source": [
        "# Encoding categorical variables\n",
        "\n",
        "# Use Pandas to encode us values as 1 and others as 0\n",
        "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda row: 1 if row == 'us' else 0)\n",
        "\n",
        "# Print the number of unique type values\n",
        "print(len(ufo.type.unique()))\n",
        "\n",
        "# Create a one-hot encoded set of the type values\n",
        "type_set = pd.get_dummies(ufo.type)\n",
        "\n",
        "# Concatenate this set back to the ufo DataFrame\n",
        "ufo = pd.concat([ufo, type_set], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDDW8eekp_eS",
        "outputId": "d7b9c4b5-f95f-4280-db30-0b86b1fb3c73"
      },
      "source": [
        "# Features from dates\n",
        "\n",
        "# Look at the first 5 rows of the date column\n",
        "print(ufo.date.head())\n",
        "\n",
        "# Extract the month from the date column\n",
        "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
        "\n",
        "# Extract the year from the date column\n",
        "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
        "\n",
        "# Take a look at the head of all three columns\n",
        "print(ufo[['date', 'month', 'year']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   2011-11-03 19:21:00\n",
            "1   2004-10-03 19:05:00\n",
            "3   2002-11-21 05:45:00\n",
            "4   2010-08-19 12:55:00\n",
            "5   2012-06-16 23:00:00\n",
            "Name: date, dtype: datetime64[ns]\n",
            "                    date  month  year\n",
            "0    2011-11-03 19:21:00     11  2011\n",
            "1    2004-10-03 19:05:00     10  2004\n",
            "3    2002-11-21 05:45:00     11  2002\n",
            "4    2010-08-19 12:55:00      8  2010\n",
            "5    2012-06-16 23:00:00      6  2012\n",
            "...                  ...    ...   ...\n",
            "4930 2000-07-05 19:30:00      7  2000\n",
            "4931 2008-03-18 22:00:00      3  2008\n",
            "4932 2005-06-15 02:30:00      6  2005\n",
            "4933 1991-11-01 03:00:00     11  1991\n",
            "4934 2005-12-10 18:00:00     12  2005\n",
            "\n",
            "[4283 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4fS8UznqMEF",
        "outputId": "ac324a20-5b78-4b92-e20d-ca229bdcd88a"
      },
      "source": [
        "# Text vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Take a look at the head of the desc field\n",
        "print(ufo['desc'].head())\n",
        "\n",
        "# Create the tfidf vectorizer object\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "# Use vec's fit_transform method on the desc field\n",
        "desc_tfidf = vec.fit_transform(ufo.desc)\n",
        "\n",
        "# Look at the number of columns this creates\n",
        "print(desc_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    Red blinking objects similar to airplanes or s...\n",
            "1                 Many fighter jets flying towards UFO\n",
            "3    It was a large&#44 triangular shaped flying ob...\n",
            "4       A white spinning disc in the shape of an oval.\n",
            "5    Dancing lights that would fly around and then ...\n",
            "Name: desc, dtype: object\n",
            "(4283, 5754)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__OjWqR_qkob",
        "outputId": "5446348b-9d19-4b8e-d739-528e4f5c1a4f"
      },
      "source": [
        "# Feature selection and modeling\n",
        "\n",
        "# Selecting the ideal dataset\n",
        "\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    \n",
        "    # Let's transform that zipped dict into a series\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    \n",
        "    # Let's sort the series to pull out the top n weighted words\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    \n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "    \n",
        "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "vocab = {v:k for k,v in vec.vocabulary_.items()}\n",
        "\n",
        "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
        "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
        "\n",
        "# Make a list of features to drop\n",
        "to_drop = ['city', 'country', 'lat','long', 'state', 'seconds', 'minutes', 'date', 'desc', 'length_of_time', 'recorded']\n",
        "\n",
        "# Drop those features\n",
        "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
        "\n",
        "# Let's also filter some words out of the text vector we created\n",
        "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              seconds  seconds_log   minutes\n",
            "seconds      1.000000     0.174331 -0.009932\n",
            "seconds_log  0.174331     1.000000  0.111460\n",
            "minutes     -0.009932     0.111460  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "W47sDQgktJ89",
        "outputId": "f1443824-e149-459e-ee51-ca9b05536bd0"
      },
      "source": [
        "# Modeling the UFO dataset, part 1\n",
        "\n",
        "ufo_dropped.columns\n",
        "X = ufo_dropped[['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
        "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
        "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
        "       'teardrop', 'triangle', 'unknown', 'month', 'year']]\n",
        "\n",
        "y = ufo_dropped['country_enc']\n",
        "\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors.classification import KNeighborsClassifier\n",
        "knn =  KNeighborsClassifier()\n",
        "\n",
        "# Take a look at the features in the X set of data\n",
        "print(X.columns)\n",
        "\n",
        "# Split the X and y sets using train_test_split, setting stratify=y\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# Fit knn to the training sets\n",
        "knn.fit(train_X, train_y)\n",
        "\n",
        "# Print the score of knn on the test sets\n",
        "print(knn.score(test_X, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
            "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
            "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
            "       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-fa3a86ee8525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Fit knn to the training sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Print the score of knn on the test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \"\"\"\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrYA_x6x1y_9",
        "outputId": "3463440d-1bc6-4fcd-91e7-9355e82321b4"
      },
      "source": [
        "# Modeling the UFO dataset, part 2\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Use the list of filtered words we created to filter the text vector\n",
        "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
        "\n",
        "# Split the X and y sets using train_test_split, setting stratify=y \n",
        "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit nb to the training sets\n",
        "nb.fit(train_X, train_y)\n",
        "\n",
        "# Print the score of nb on the test sets\n",
        "print(nb.score(test_X, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6302521008403361\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}