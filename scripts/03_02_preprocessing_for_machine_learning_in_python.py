# -*- coding: utf-8 -*-
"""03-02-preprocessing-for-machine-learning-in-python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S0wXXAccDTHXEf4jMZiFdjrhIojeV3G4
"""

#Preprocessing for Machine Learning in Python
## 1. Introduction to Data Preprocessing
## 2. Standardizing Data
## 3. Feature Engineering
## 4. Selecting features for modeling
## 5. Putting it all together

"""## 1. Introduction to Data Preprocessing"""

# What is data preprocessing?

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)
print(volunteer.shape)

volunteer = volunteer.dropna(axis=1, thresh=3)
print(volunteer.shape)

"""**Missing data - columns**

We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.

How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?

- The dataset volunteer has been provided.
- Use the dropna() function to remove columns.
- You'll have to set both the axis= and thresh= parameters.

**Possible Answers**

- [x] 35, 24
- [ ] 35, 35
- [ ] 35, 19
"""

# Missing data - rows

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)

# Check how many values are missing in the category_desc column
print(volunteer['category_desc'].isnull().sum())
volunteer['category_desc'].head()

# Subset the volunteer dataset
volunteer_subset = volunteer[volunteer['category_desc'].notnull()]

# Print out the shape of the subset
print(volunteer_subset.shape)

# Working with data types

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)
volunteer.dtypes

"""**Exploring data types**

Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we'll be working with as we start to do more preprocessing.

Which data types are present in the volunteer dataset?

The dataset volunteer has been provided.
Use the .dtypes attribute to check the datatypes.

**Possible Answers**

- [ ] Float and int only
- [ ] Int only
- [x] Float, int, and object
- [ ] Float only
"""

# Converting a column type

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)

# Print the head of the hits column
print(volunteer['hits'].head())

# Convert the hits column to type int
volunteer['hits'] = volunteer['hits'].astype(int)

# Look at the dtypes of the dataset
print(volunteer.dtypes)

# Class distribution

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)
volunteer['category_desc'].value_counts()

"""**Class imbalance**

In the volunteer dataset, we're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.

Which descriptions occur less than 50 times in the volunteer dataset?

- The dataset volunteer has been provided.
- The colum you want to check is category_desc.
- Use the value_counts() method to check variable counts.

**Possible Answers**

- [ ] Emergency Preparedness
- [ ] Health
- [ ] Environment
- [x] 1 and 3
- [ ] All of the above
"""

# Stratified sampling

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)
volunteer = volunteer[volunteer['category_desc'].notnull()]

from sklearn.model_selection import train_test_split

# Create a data with all columns except category_desc
volunteer_X = volunteer.drop("category_desc", axis=1)

# Create a category_desc labels dataset
volunteer_y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset according to the volunteer_y dataset
X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)

# Print out the category_desc counts on the training y labels
print(y_train["category_desc"].value_counts())

"""## 2. Standardizing Data

**When to standardize**

Now that you've learned when it is appropriate to standardize your data, which of these scenarios would you NOT want to standardize?

**Possible Answers**

- [ ] A column you want to use for modeling has extremely high variance.
- [ ] You have a dataset with several continuous columns on different scales and you'd like to use a linear model to train the data.
- [ ] The models you're working with use some sort of distance metric in a linear space, like the Euclidean metric.
- [x] Your dataset is comprised of categorical data.
"""

# Modeling without normalizing

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

y = wine.Type
X = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

# Log normalization

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)
wine.var()

"""**Checking the variance**

Check the variance of the columns in the wine dataset. Out of the four columns listed in the multiple choice section, which column is a candidate for normalization?

**Possible Answers**

- [ ] Alcohol
- [x] Proline (Has an extremely high variance.)
- [ ] Proanthocyanins
- [ ] Ash
"""

# Log normalization in Python

import numpy as np
import pandas as pd

filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

# Print out the variance of the Proline column
print(wine.Proline.var())

# Apply the log normalization function to the Proline column
wine['Proline_log'] = np.log(wine.Proline)

# Check the variance of the Proline column again
print(wine.Proline_log.var())

# Scaling data for feature comparison

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)
wine.describe()

"""**Scaling data - investigating columns**

We want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. 

Using describe() to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?

**Possible Answers**

- [ ] The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162.
- [ ] The means of Ash and Alcalinity of ash are less than 20, while the mean of Magnesium is greater than 90.
- [ ] The standard deviations of Ash and Alcalinity of ash are equal.
- [x] 1 and 2 are true
"""

# Scaling data - standardizing columns

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

# Import StandardScaler from scikit-learn
from sklearn.preprocessing import StandardScaler

# Create the scaler
ss = StandardScaler()

# Take a subset of the DataFrame you want to scale 
wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]

# Apply the scaler to the DataFrame subset
wine_subset_scaled = ss.fit_transform(wine_subset)

# Standardized data and modeling

# KNN on non-scaled data

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

X = wine.drop('Type', axis=1)
y = wine.Type

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

# KNN on scaled data

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

X = wine.drop('Type', axis=1)
y = wine.Type

from sklearn.model_selection import train_test_split

# Import StandardScaler from scikit-learn
from sklearn.preprocessing import StandardScaler

# Create the scaling method.
ss = StandardScaler()

# Apply the scaling method to the dataset used for modeling.
X_scaled = ss.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Fit the k-nearest neighbors model to the training data.
knn.fit(X_train, y_train)

# Score the model on the test data.
print(knn.score(X_test, y_test))

"""## 3. Feature Engineering

**Feature engineering knowledge test**

Now that you've learned about feature engineering, which of the following examples are good candidates for creating new features?

**Possible Answers**

- [ ] A column of timestamps
- [ ] A column of newspaper headlines
- [ ] A column of weight measurements
- [x] 1 and 2
- [ ] None of the above

**Identifying areas for feature engineering**

Take an exploratory look at the volunteer dataset, using the variable of that name. 

Which of the following columns would you want to perform a feature engineering task on?

**Possible Answers**

- [ ] vol_requests
- [ ] title
- [ ] created_date
- [ ] category_desc
- [x] 2, 3, and 4
"""

# Encoding categorical variables

# Encoding categorical variables - binary

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'
hiking = pd.read_json(filename)

from sklearn.preprocessing import LabelEncoder

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])

# Compare the two columns
print(hiking[['Accessible', 'Accessible_enc']].head())

# Encoding categorical variables - one-hot

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['category_desc'])

# Take a look at the encoded columns
print(category_enc.head())

# Engineering numerical features

# Engineering numerical features - taking an average

data = {'name': ['Sue', 'Mark', 'Sean', 'Erin', 'Jenny', 'Russell'],
        'run1': [20.1, 16.5, 23.5, 21.7, 25.8, 30.9],
        'run2': [18.5, 17.1, 25.1, 21.1, 27.1, 29.6],
        'run3': [19.6, 16.9, 25.2, 20.9, 26.1, 31.4],
        'run4': [20.3, 17.6, 24.6, 22.1, 26.7, 30.4],
        'run5': [18.3, 17.3, 23.9, 22.2, 26.9, 29.9]}
running_times_5k = pd.DataFrame.from_dict(data)

# Create a list of the columns to average
run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']

# Use apply to create a mean column
running_times_5k["mean"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)

# Take a look at the results
print(running_times_5k)

# Engineering numerical features - datetime

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)

# First, convert string column to date column
volunteer["start_date_converted"] = pd.to_datetime(volunteer['start_date_date'])

# Extract just the month from the converted column
volunteer["start_date_month"] = volunteer['start_date_converted'].apply(lambda row: row.month)

# Take a look at the original and new columns
print(volunteer[['start_date_converted', 'start_date_month']].head())

# Text classification

# Engineering features from strings - extraction

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'
hiking = pd.read_json(filename)
hiking = hiking.dropna(subset=['Length'])

import re

# Write a pattern to extract numbers and decimals
def return_mileage(length):
    pattern = re.compile("\d+\.\d+")
    print(length)
    # Search the text for matches
    mile = re.match(pattern, length)
    
    # If a value is returned, use group(0) to return the found value
    if mile is not None:
        return float(mile.group(0))

      
# Apply the function to the Length column and take a look at both columns
hiking["Length_num"] = hiking['Length'].apply(lambda row: return_mileage(row))
print(hiking[["Length", "Length_num"]].head())

# Engineering features from strings - tf/idf

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)

from sklearn.feature_extraction.text import TfidfVectorizer

# Take the title text
title_text = volunteer['title']

# Create the vectorizer method
tfidf_vec = TfidfVectorizer()

# Transform the text into tf-idf vectors
text_tfidf = tfidf_vec.fit_transform(title_text)

# Text classification using tf/idf vectors

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])
volunteer = volunteer.dropna(subset=['category_desc'])

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_extraction.text import TfidfVectorizer

title_text = volunteer['title']
tfidf_vec = TfidfVectorizer()
text_tfidf = tfidf_vec.fit_transform(title_text)

nb = GaussianNB()

# Split the dataset according to the class distribution of category_desc
y = volunteer["category_desc"]
X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)

# Fit the model to the training data
nb.fit(X_train, y_train)

# Print out the model's accuracy
print(nb.score(X_test, y_test))

"""## 4. Selecting features for modeling"""

# Feature selection

"""**When to use feature selection**

Let's say you had finished standardizing your data and creating new features. 

Which of the following scenarios is NOT a good candidate for feature selection?

**Possible Answers**

- [ ] Several columns of running times that have been averaged into a new column.
- [x] A text field that hasn't been turned into a tf/idf vector yet.
- [ ] A column of text that has already had a float extracted out of it.
- [ ] A categorical field that has been one-hot encoded.
- [ ] Your dataset contains columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant.
"""

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/hiking.json'
hiking = pd.read_json(filename)

"""**Identifying areas for feature selection**

Take an exploratory look at the post-feature engineering hiking dataset. 

Which of the following columns is a good candidate for feature selection?

**Possible Answers**

- [ ] Length
- [ ] Difficulty
- [ ] Accessible
- [x] All of the above
- [ ] None of the above
"""

# Removing redundant features

# Selecting relevant features

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename)
volunteer = volunteer.dropna(subset=['category_desc'])

# Create a list of redundant column names to drop
to_drop = ["category_desc", "created_date", "locality", "region", "vol_requests"]

# Drop those columns from the dataset
volunteer_subset = volunteer.drop(to_drop, axis=1)

# Print out the head of the new dataset
print(volunteer_subset.head())

# Checking for correlated features

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename, usecols=['Flavanoids', 'Total phenols', 'Malic acid', 'OD280/OD315 of diluted wines', 'Hue'])
wine = wine.reindex(columns=['Flavanoids', 'Total phenols', 'Malic acid', 'OD280/OD315 of diluted wines', 'Hue'])

# Print out the column correlations of the wine dataset
print(wine.corr())

# Take a minute to find the column where the correlation value is greater than 0.75 at least twice
to_drop = "Flavanoids"

# Drop that column from the DataFrame
wine = wine.drop(to_drop, axis=1)

# Selecting features using text vectors

# Exploring text vectors, part 1

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])
volunteer = volunteer.dropna(subset=['category_desc'])

from sklearn.feature_extraction.text import TfidfVectorizer
title_text = volunteer['title']
tfidf_vec = TfidfVectorizer()
text_tfidf = tfidf_vec.fit_transform(title_text)
vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}

# Add in the rest of the parameters
def return_weights(vocab, original_vocab, vector, vector_index, top_n):
    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))
    
    # Let's transform that zipped dict into a series
    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})
    
    # Let's sort the series to pull out the top n weighted words
    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index
    
    return [original_vocab[i] for i in zipped_index]

# Print out the weighted words
print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))

# Exploring text vectors, part 2

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/volunteer_opportunities.csv'
volunteer = pd.read_csv(filename, usecols=['category_desc', 'title'])
volunteer = volunteer.dropna(subset=['category_desc'])

from sklearn.feature_extraction.text import TfidfVectorizer
title_text = volunteer['title']
tfidf_vec = TfidfVectorizer()
text_tfidf = tfidf_vec.fit_transform(title_text)
vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}

def words_to_filter(vocab, original_vocab, vector, top_n):
    filter_list = []
    for i in range(0, vector.shape[0]):
    
        # Here we'll call the function from the previous exercise, and extend the list we're creating
        filtered = return_weights(vocab, original_vocab, vector, i, top_n)
        filter_list.extend(filtered)
    # Return the list in a set, so we don't get duplicate word indices
    return set(filter_list)

# Call the function to get the list of word indices
filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)

# By converting filtered_words back to a list, we can use it to filter the columns in the text vector
filtered_text = text_tfidf[:, list(filtered_words)]

# Training Naive Bayes with feature selection

from sklearn.naive_bayes import GaussianNB
np = GaussianNB()

# Split the dataset according to the class distribution of category_desc, using the filtered_text vector
train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)

# Fit the model to the training data
nb.fit(train_X, train_y)

# Print out the model's accuracy
print(nb.score(test_X, test_y))

# Dimensionality reduction

# Using PCA

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/wine_types.csv'
wine = pd.read_csv(filename)

from sklearn.decomposition import PCA

# Set up PCA and the X vector for diminsionality reduction
pca = PCA()
wine_X = wine.drop("Type", axis=1)

# Apply PCA to the wine dataset X vector
transformed_X = pca.fit_transform(wine_X)

# Look at the percentage of variance explained by the different components
print(pca.explained_variance_ratio_)

# Training a model with PCA

from sklearn.naive_bayes import GaussianNB
knn = GaussianNB()

y = wine.dropna().Type
transformed_X.shape

# Split the transformed X and the y labels into training and test sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)

# Fit knn to the training data
knn.fit(X_wine_train, y_wine_train)

# Score knn on the test data and print it out
print(knn.score(X_wine_test, y_wine_test))

"""## 5. Putting it all together"""

# UFOs and preprocessing

# Checking column types

import pandas as pd
filename = 'https://raw.githubusercontent.com/chesterheng/ai-for-industry/main/datasets/ufo_sightings_large.csv'
ufo = pd.read_csv(filename, parse_dates=['date'])

# Check the column types
print(ufo.dtypes)

# Change the type of seconds to float
ufo["seconds"] = ufo['seconds'].astype('float')

# Change the date column to type datetime
ufo["date"] = pd.to_datetime(ufo['date'])

# Check the column types
print(ufo[['seconds', 'date']].dtypes)

# Dropping missing data

# Check how many values are missing in the length_of_time, state, and type columns
print(ufo[['length_of_time', 'state', 'type']].isnull().sum())

# Keep only rows where length_of_time, state, and type are not null
ufo_no_missing = ufo[
        ufo['length_of_time'].notnull() & 
        ufo['state'].notnull() & 
        ufo['type'].notnull()
]

# Print out the shape of the new dataset
print(ufo_no_missing.shape)

# Categorical variables and standardization

ufo = ufo_no_missing.copy()
import re

# Extracting numbers from strings

def return_minutes(time_string):
    
    # We'll use \d+ to grab digits and match it to the column values
    pattern = re.compile(r"\d+")
        
    # Use match on the pattern and column
    num = re.match(pattern, time_string)
    if num is not None:
        return int(num.group(0))
        
# Apply the extraction to the length_of_time column
ufo["minutes"] = ufo["length_of_time"].apply(return_minutes)

# Take a look at the head of both of the columns
print(ufo[["length_of_time", "minutes"]].head())

# Identifying features for standardization

ufo.var()
import numpy as np

# Check the variance of the seconds and minutes columns
print(ufo[['seconds','minutes']].var())

# Log normalize the seconds column
ufo["seconds_log"] = np.log(ufo[['seconds']])

# Print out the variance of just the seconds_log column
print(ufo["seconds_log"].var())

# Encoding categorical variables

# Use Pandas to encode us values as 1 and others as 0
ufo["country_enc"] = ufo["country"].apply(lambda row: 1 if row == 'us' else 0)

# Print the number of unique type values
print(len(ufo.type.unique()))

# Create a one-hot encoded set of the type values
type_set = pd.get_dummies(ufo.type)

# Concatenate this set back to the ufo DataFrame
ufo = pd.concat([ufo, type_set], axis=1)

# Features from dates

# Look at the first 5 rows of the date column
print(ufo.date.head())

# Extract the month from the date column
ufo["month"] = ufo["date"].dt.month

# Extract the year from the date column
ufo["year"] = ufo["date"].dt.year

# Take a look at the head of all three columns
print(ufo[['date', 'month', 'year']])

# Text vectorization

from sklearn.feature_extraction.text import TfidfVectorizer

# Take a look at the head of the desc field
print(ufo['desc'].head())

# Create the tfidf vectorizer object
vec = TfidfVectorizer()

# Use vec's fit_transform method on the desc field
desc_tfidf = vec.fit_transform(ufo.desc)

# Look at the number of columns this creates
print(desc_tfidf.shape)

# Feature selection and modeling

# Selecting the ideal dataset

def return_weights(vocab, original_vocab, vector, vector_index, top_n):
    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))
    
    # Let's transform that zipped dict into a series
    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})
    
    # Let's sort the series to pull out the top n weighted words
    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index
    
    return [original_vocab[i] for i in zipped_index]

def words_to_filter(vocab, original_vocab, vector, top_n):
    filter_list = []
    for i in range(0, vector.shape[0]):
    
        # Here we'll call the function from the previous exercise, and extend the list we're creating
        filtered = return_weights(vocab, original_vocab, vector, i, top_n)
        filter_list.extend(filtered)
    # Return the list in a set, so we don't get duplicate word indices
    return set(filter_list)

vocab = {v:k for k,v in vec.vocabulary_.items()}

# Check the correlation between the seconds, seconds_log, and minutes columns
print(ufo[['seconds', 'seconds_log', 'minutes']].corr())

# Make a list of features to drop
to_drop = ['city', 'country', 'lat','long', 'state', 'seconds', 'minutes', 'date', 'desc', 'length_of_time', 'recorded']

# Drop those features
ufo_dropped = ufo.drop(to_drop, axis=1)

# Let's also filter some words out of the text vector we created
filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)

# Modeling the UFO dataset, part 1

ufo_dropped.columns
X = ufo_dropped[['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',
       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',
       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',
       'teardrop', 'triangle', 'unknown', 'month', 'year']]

y = ufo_dropped['country_enc']

X.shape, y.shape

from sklearn.model_selection import train_test_split
from sklearn.neighbors.classification import KNeighborsClassifier
knn =  KNeighborsClassifier()

# Take a look at the features in the X set of data
print(X.columns)

# Split the X and y sets using train_test_split, setting stratify=y
train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)

# Fit knn to the training sets
knn.fit(train_X, train_y)

# Print the score of knn on the test sets
print(knn.score(test_X, test_y))

# Modeling the UFO dataset, part 2

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

# Use the list of filtered words we created to filter the text vector
filtered_text = desc_tfidf[:, list(filtered_words)]

# Split the X and y sets using train_test_split, setting stratify=y 
train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)

# Fit nb to the training sets
nb.fit(train_X, train_y)

# Print the score of nb on the test sets
print(nb.score(test_X, test_y))